{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#\u5982\u4f55\u5728CASIA50(CASIA\u7684\u5b50\u96c6)\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u4e00\u4e2aKNN\u5206\u7c7b\u5668\n",
      "CASIA50\u91cc\u8fb9\u67094\u4e2a\u8bf4\u8bdd\u4eba,50\u4e2a\u6587\u672c,\u516d\u79cd\u60c5\u611f,\u6240\u4ee5\u8fd9\u4e2a\u6570\u636e\u96c6\u4e00\u51711200\u6761\u53e5.\n",
      "KNN\u5206\u7c7b\u5668\u662f\u6700\u7b80\u5355\u7684\u5206\u7c7b\u5668,\u4ee5\u6b64\u4f5c\u4e3a\u5b9e\u9a8cbaseline.\u4e0b\u9762\u4ece\u7279\u5f81\u63d0\u53d6\u5f00\u59cb,\u4e00\u6b65\u4e00\u6b65\u8bad\u7ec3\u8fd9\u4e2a\u5206\u7c7b\u5668(\u4f46\u5b9e\u9645\u4e0aKNN\u7b80\u5355\u5230\u90fd\u4e0d\u80fd\u7528\u8bad\u7ec3\u8fd9\u4e2a\u8bcd).\n",
      "\u505a\u8fd9\u4e2a\u5b9e\u9a8c\u4f60\u9700\u8981\n",
      "python, python-numpy, python-cPickle, python-\n",
      "##1.\u7279\u5f81\u63d0\u53d6\n",
      "\u9996\u5148\u89e3\u538b\u7f29CASIA50\u8bed\u6599\u5e93,\u6ce8\u610f\u4fee\u6539\u6587\u4ef6\u76ee\u5f55,\u4e0d\u8981\u8ba9\u76ee\u5f55\u91cc\u5e26\u4e2d\u6587,\u4e0d\u7136\u53ef\u80fd\u4f1a\u51fa\u4e00\u70b9\u95ee\u9898.\u6587\u4ef6\u5939\u7ed3\u6784\u5927\u6982\u5982\u4e0b"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      ".\n",
      "\u251c\u2500\u2500 liuchanhg\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 angry\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.lab\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.peak\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.tag\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.wav\n",
      ".\n",
      ".\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 fear\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.lab\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.peak\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.tag\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.wav\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 202.lab\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 202.peak\n",
      ".\n",
      ".\n",
      "\u251c\u2500\u2500 wangzhe\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 angry\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.lab\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.peak\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.tag\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 201.wav\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 202.lab\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 202.peak\n",
      "\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 202.tag\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      " \n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u7136\u540e\u9700\u8981\u4e00\u4e9b\u811a\u672c\u63d0\u53d6\u8fd9\u4e9bwav\u6587\u4ef6\u7684\u8def\u5f84,\u5e76\u4e14\u8981\u8bb0\u4e0bwav\u6587\u4ef6\u7684\u6807\u7b7e,\u4f5c\u4e3a\u4ee5\u540e\u8bad\u7ec3\u65f6\u7684\u6807\u7b7e.\u4ece\u6587\u4ef6\u5939\u548c\u6587\u4ef6\u540d\u5c31\u80fd\u770b\u51fawav\u6587\u4ef6\u7684\u6807\u7b7e\u4e86,\u6bd4\u5982\u53ef\u4ee5\u7528liuchanhg_angry_201\u8868\u793a\u4e00\u4e2awav."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# extract_file_path.py\n",
      "import os\n",
      "import os.path\n",
      "import cPickle\n",
      "rootDir = '/home/hehe/Corpus/emotion/dataset_610677/casia50/50' #\u8fd9\u4e2a\u8981\u6362\u6210\u4f60\u81ea\u5df1\u7684\u8bed\u6599\u5e93\u8def\u5f84\n",
      "filelist = []\n",
      "for (dirname, dirs, files) in os.walk(rootDir):\n",
      "    for file in files:\n",
      "        if file.endswith('.wav'):\n",
      "            #print file\n",
      "            gotfile = os.path.join(os.path.abspath(dirname), file)\n",
      "            filelist.append(gotfile)\n",
      "filelist.sort()\n",
      "with open(os.path.join(rootDir,'wav_path.txt'), 'w') as f:#\u5c06\u6587\u4ef6\u8def\u5f84\u4fdd\u5b58\u6210\u4e00\u4e2atxt\u6587\u4ef6\n",
      "\tfor line in filelist:\n",
      "\t\tf.writelines(line+'\\n')\n",
      "cPickle.dump(filelist, open(os.path.join(RootDir, 'filelist.pkl'), 'w')) #\u4fdd\u5b58\u4e00\u4efdpython\u683c\u5f0f\u4ee5\u4fbf\u4e0b\u4e00\u6b65\u8bfb\u53d6\u65b9\u4fbf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u7136\u540e\u628awav\u6807\u7b7e\u4e5f\u63d0\u53d6\u7136\u540e\u4fdd\u5b58\u4e0b:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse_name_emo.py \n",
      "import cPickle as Pickle\n",
      "import re\n",
      "import os.path\n",
      "import os\n",
      "rootDir = '/home/hehe/Corpus/emotion/dataset_610677/casia50/50'  #\u8fd9\u4e2a\u8981\u6362\u6210\u4f60\u81ea\u5df1\u7684\u8bed\u6599\u5e93\u8def\u5f84\n",
      "\n",
      "filelist = Pickle.load(open(os.path.join(rootDir,'filelist.pkl'), 'rb'))\n",
      "name_category_num = []\n",
      "name_category_num_lst = []\n",
      "\n",
      "for filepath in filelist:\n",
      "\tfilepath_split = re.split(r'[/.]', filepath)\n",
      "\tname_category_num.append('_'.join(filepath_split[-4:-1]))\n",
      "\ttemp = []\n",
      "\ttemp.append(filepath_split[-4])\n",
      "\ttemp.append(filepath_split[-3])\n",
      "\ttemp.append(filepath_split[-2])\n",
      "\tname_category_num_lst.append(temp)\n",
      "with open(os.path.join(rootDir,'name_category_num.txt'), 'w') as f: #\u5199\u4e2atxt\n",
      "\tfor line in name_category_num:\n",
      "\t\tf.writelines(line+'\\n')\n",
      "Pickle.dump(name_category_num_lst, open(os.path.join(rootDir,'name_category_num_lst.pkl'), 'wb'))#\u518d\u5199\u4efdpython\u6570\u636e\u683c\u5f0f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u7136\u540e\u5f00\u59cb\u63d0\u53d6\u7279\u5f81,\u4f7f\u7528openSMILE.openSMILE\u57fa\u672c\u547d\u4ee4\u5982\u4e0b(\u524d\u63d0\u662f\u4f60\u8981\u5b89\u88c5\u597dopenSMILE,\u5e76\u628a\u5b83\u52a0\u5165\u7cfb\u7edfpath):\n",
      "\n",
      "SMILExtract -C config_file -I input_file -O output_file -classes {angry,fear,happy,neutral,sad,surprise} -classlabel angry -instname file_tag\n",
      "\n",
      "config_file\u662f\u914d\u7f6e\u6587\u4ef6,\u5c31\u662f\u544a\u8bc9openSMILE\u8981\u63d0\u53d6\u4ec0\u4e48\u6837\u7684\u7279\u5f81,\u6211\u4eec\u76f4\u63a5\u7528\u5b83\u63d0\u4f9b\u768409\u5e74\u7684feature set\u914d\u7f6e\u6587\u4ef6,\u4ee5\u540e\u8bfb\u61c2\u5b83\u90fd\u63d0\u53d6\u4e86\u4e9b\u4ec0\u4e48\u4e1c\u897f.input_file\u5c31\u662f\u8f93\u5165wav,output_file\u5c31\u662f\u8f93\u51fa\u6587\u4ef6,openSMILE\u53ef\u4ee5\u628a\u597d\u591a\u8f93\u5165\u90fd\u8f93\u51fa\u5230\u540c\u4e00\u4e2a\u6587\u4ef6,\u65b9\u4fbf\u4ee5\u540e\u8bfb\u53d6.-classlabel\u662f\u60c5\u611f\u6807\u7b7e,\u6211\u4eec\u7528\u7684\u662f6\u79cd\u79bb\u6563\u60c5\u611f.-instname\u662f\u6587\u4ef6\u6807\u7b7e,\u4f5c\u4e3a\u4e00\u4e2awav\u6587\u4ef6\u7684\u552f\u4e00\u6807\u7b7e\u8bb0\u5f55\u4e0b\u6765."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# extract_with_opensmile.py\n",
      "import os\n",
      "import os.path\n",
      "import cPickle\n",
      "rootDir = '/home/hehe/Corpus/emotion/dataset_610677/casia50/50'  #\u8fd9\u4e2a\u8981\u6362\u6210\u4f60\u81ea\u5df1\u7684\u8bed\u6599\u5e93\u8def\u5f84\n",
      "\n",
      "configFile = 'emo_IS09.conf'\n",
      "\n",
      "outputFile = 'casia50_emo_IS09.arff'\n",
      "#load list\n",
      "filelist = cPickle.load(open(os.path.join(rootDir,'filelist.pkl'), 'rb'))\n",
      "name_cat_num = cPickle.load(open(os.path.join(rootDir,'name_category_num_lst.pkl'), 'rb'))\n",
      "#generate instruction and exec it\n",
      "i = 0\n",
      "for filepath, name in zip(filelist, name_cat_num):\n",
      "\tinstruction = 'SMILExtract -C '+ os.path.join(rootDir, configFile)\\\n",
      "\t+' -I '+filepath+' -O '+os.path.join(rootDir, outputFile)\\\n",
      "\t+' -classes {angry,fear,happy,neutral,sad,surprise} -classlabel '+name[1]\\\n",
      "\t+' -instname '+'_'.join(name)\n",
      "\tos.system(instruction)\n",
      "\ti+=1\n",
      "print str(i) + ' done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1200 done\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u8ba1\u7b97\u9700\u8981\u5927\u6982\u4e00\u5206\u949f,\u5b8c\u4e86\u4e4b\u540e\u53ef\u4ee5\u6253\u5f00\u5b83\u770b\u770b\u683c\u5f0f,\u662fARFF\u683c\u5f0f.\n",
      "\u4e4b\u540e\u9700\u8981\u628a\u521a\u624d\u8ba1\u7b97\u7684\u7279\u5f81\u5199\u6210python\u7684numpy\u683c\u5f0f,\u4e5f\u5c31\u662f\u6570\u7ec4\u683c\u5f0f."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# format_dataset.py\n",
      "import numpy as np\n",
      "import cPickle\n",
      "import os\n",
      "import os.path\n",
      "import arff\n",
      "#load arff\n",
      "rootDir = '/home/hehe/Corpus/emotion/dataset_610677/casia50/50'  #\u8fd9\u4e2a\u8981\u6362\u6210\u4f60\u81ea\u5df1\u7684\u8bed\u6599\u5e93\u8def\u5f84\n",
      "\n",
      "arff_name = 'casia50_emo_IS09.arff'\n",
      "file = open(os.path.join(rootDir, arff_name), \"rb\")\n",
      "dataset = arff.load(file)\n",
      "all_data = dataset.get('data')\n",
      "emo_feat = []\n",
      "emo_tag = []\n",
      "for one_data in all_data:\n",
      "    emo_feat.append(one_data[2:-1]) #first is name, second is frameNumber, last is label\n",
      "    emo_tag.append([one_data[0], one_data[-1]])\n",
      "\n",
      "emo_feature = np.asarray(emo_feat)\n",
      "emo_tag = np.asarray(emo_tag)\n",
      "cPickle.dump(emo_feature, open(os.path.join(rootDir, 'emotion_feature_np.pkl'), 'w'))#\u4fdd\u5b58\u7279\u5f81\n",
      "cPickle.dump(emo_tag, open(os.path.join(rootDir, 'emotion_tags.pkl'), 'w'))#\u4fdd\u5b58\u6807\u7b7e\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u7279\u5f81\u96c6\u51c6\u5907\u5b8c\u4e86,\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3\u4e86\n",
      "##KNN\u8bad\u7ec3\n",
      "knn\u6a21\u578b\u4ee3\u7801\u6765\u81eaBuilding Machine Learning Systems with Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# knn.py\n",
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# This function was called ``learn_model`` in the first edition\n",
      "def fit_model(k, features, labels):\n",
      "    '''Learn a k-nn model'''\n",
      "    # There is no model in k-nn, just a copy of the inputs\n",
      "    return k, features.copy(), labels.copy()\n",
      "\n",
      "\n",
      "def plurality(xs):\n",
      "    '''Find the most common element in a collection'''\n",
      "    from collections import defaultdict\n",
      "    counts = defaultdict(int)\n",
      "    for x in xs:\n",
      "        counts[x] += 1\n",
      "    maxv = max(counts.values())\n",
      "    for k, v in counts.items():\n",
      "        if v == maxv:\n",
      "            return k\n",
      "\n",
      "# This function was called ``apply_model`` in the first edition\n",
      "def predict(features, model):\n",
      "    '''Apply k-nn model'''\n",
      "    k, train_feats, labels = model\n",
      "    results = []\n",
      "    for f in features:\n",
      "        label_dist = []\n",
      "        # Compute all distances:\n",
      "        for t, ell in zip(train_feats, labels):\n",
      "            label_dist.append((np.linalg.norm(f - t), ell))\n",
      "        label_dist.sort(key=lambda d_ell: d_ell[0])\n",
      "        label_dist = label_dist[:k]\n",
      "        results.append(plurality([ell for _, ell in label_dist]))\n",
      "    return np.array(results)\n",
      "\n",
      "\n",
      "def accuracy(features, labels, model):\n",
      "    preds = predict(features, model)\n",
      "    return np.mean(preds == labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u4f7f\u7528\u4e8610\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6cd5\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u5212\u5206,\u5177\u4f53\u53c2\u8003Building Machine Learning Systems with Python\u76842.1\u8282.\u5206\u522b\u8ba1\u7b97k\u4ece1\u523010\u7684\u6b63\u786e\u7387."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# casia_knn.py\n",
      "import cPickle\n",
      "import numpy as np\n",
      "import os.path\n",
      "#from knn import fit_model, accuracy  #\u5355\u72ec\u8fd0\u884c\u7684\u65f6\u5019\u628a\u8fd9\u884c\u6ce8\u91ca\u53bb\u6389\n",
      "rootDir = '/home/hehe/Corpus/emotion/dataset_610677/casia50/50'  #\u8fd9\u4e2a\u8981\u6362\u6210\u4f60\u81ea\u5df1\u7684\u8bed\u6599\u5e93\u8def\u5f84\n",
      "features = cPickle.load(open(os.path.join(rootDir, 'emotion_feature_np.pkl'), 'r'))\n",
      "tags = cPickle.load(open(os.path.join(rootDir, 'emotion_tags.pkl'), 'r'))\n",
      "                    \n",
      "labels = []\n",
      "for i in tags:\n",
      "    labels.append(i[1])\n",
      "labels = np.asarray(labels)  \n",
      "def cross_validate(k, features, labels):\n",
      "    '''Compute cross-validation errors'''\n",
      "    error = 0.0\n",
      "    for fold in range(10):\n",
      "        training = np.ones(len(features), bool)\n",
      "        training[fold::10] = 0\n",
      "        testing = ~training\n",
      "        model = fit_model(k, features[training], labels[training])\n",
      "        test_error = accuracy(features[testing], labels[testing], model)\n",
      "        error += test_error\n",
      "\n",
      "    return error / 10.0\n",
      "for k in xrange(10):\n",
      "    error = cross_validate(k+1, features, labels)\n",
      "    print error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.3175\n",
        "0.3025"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.315"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.316666666667"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.318333333333"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.321666666667"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.3375"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.328333333333"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.328333333333"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}